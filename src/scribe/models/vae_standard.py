"""
VAE-based models for single-cell RNA sequencing data using standard
parameterization.

This parameterization uses a Variational Autoencoder (VAE) architecture to
generate cell-specific dispersion parameters (r) from a low-dimensional latent
space, while maintaining global parameters (p) as interpretable global
variables.

The VAE architecture consists of:
    1. A global parameter p sampled from a Beta prior distribution
    2. A latent variable z sampled from a standard Normal prior
    3. A decoder network that maps z to cell-specific r parameters
    4. The Negative Binomial likelihood using VAE-generated r and global p

This approach allows for modeling cell-to-cell heterogeneity in dispersion while
maintaining interpretable global parameters. The VAE learns meaningful
representations of the data in the latent space, which can be useful for
downstream analysis tasks such as clustering and visualization.

Parameters are sampled from appropriate prior distributions:
    - p is the success probability (sampled from Beta distribution)
    - r is the dispersion parameter (generated by VAE decoder)
    - The relationship is: counts ~ NegativeBinomialProbs(r, p)

This module implements the STANDARD parameterization where:
    - p is the success probability (sampled from Beta prior)
    - r is the dispersion parameter (generated by VAE decoder)
    - The relationship is: counts ~ NegativeBinomialProbs(r, p)
"""

import jax
import jax.numpy as jnp
import numpyro
import numpyro.distributions as dist
from numpyro.distributions import constraints
from numpyro.contrib.module import nnx_module
from typing import Dict, Optional

from .model_config import ModelConfig
from ..vae.architectures import (
    Encoder,
    EncoderVCP,
    Decoder,
    VAE,
    DecoupledPrior,
    DecoupledPriorDistribution,
)


# ------------------------------------------------------------------------------
# NBDM VAE Model
# ------------------------------------------------------------------------------


def nbdm_vae_model(
    n_cells: int,
    n_genes: int,
    model_config: ModelConfig,
    decoder: Decoder,  # Pre-created decoder passed as argument
    counts=None,
    batch_size=None,
):
    """
    Implements the VAE-based Negative Binomial-Dirichlet Multinomial (NBDM)
    model using standard parameterization suitable for variational inference in
    single-cell RNA sequencing data.

    This model extends the standard NBDM model by incorporating a Variational
    Autoencoder (VAE) architecture. The VAE generates cell-specific dispersion
    parameters (r) from a low-dimensional latent space, while maintaining the
    global success probability parameter (p) as an interpretable global
    parameter.

    The model architecture consists of:
        1. A global parameter p sampled from a Beta prior distribution
        2. A latent variable z sampled from a standard Normal prior
        3. A decoder network that maps z to cell-specific r parameters
        4. The Negative Binomial likelihood using VAE-generated r and global p

    For each cell, the observed counts vector (of length n_genes) is modeled as:

        counts[cell, :] ~ NegativeBinomialProbs(r[cell, :], p)

    where r[cell, :] is generated by the VAE decoder from the latent variable z,
    and p is the global success probability shared across all cells.

    The VAE architecture allows the model to:
        - Capture cell-to-cell heterogeneity in dispersion parameters
        - Learn meaningful low-dimensional representations of the data
        - Maintain interpretability through global parameters
        - Scale to large datasets through efficient latent space modeling

    The model supports optional batching over cells for scalable inference. If
    `counts` is provided, it is used as observed data; otherwise, the model
    samples counts from the generative process.

    Parameters
    ----------
    n_cells: int
        Number of cells in the dataset.
    n_genes: int
        Number of genes (features) per cell.
    model_config: ModelConfig
        ModelConfig object specifying prior parameters and VAE configuration.
    decoder: Decoder
        Pre-trained decoder network that maps latent variables to r parameters.
    counts: Optional[jnp.ndarray], default=None
        Observed count data (not used in the model, but included for interface
        compatibility).
    batch_size: Optional[int], default=None
        If specified, enables subsampling of cells for stochastic variational
        inference.

    Returns
    -------
    None
        This function defines the probabilistic model for use with NumPyro.
    """
    # Define prior parameters
    p_prior_params = model_config.p_param_prior or (1.0, 1.0)

    # Sample global success probability p from Beta prior
    p = numpyro.sample("p", dist.Beta(*p_prior_params))

    # Register the pre-created decoder as NumPyro module
    decoder_module = nnx_module("decoder", decoder)

    # Sample counts
    if counts is not None:
        if batch_size is None:
            # Without batching: sample counts for all cells
            with numpyro.plate("cells", n_cells):
                # Sample from latent space prior
                z = numpyro.sample(
                    "z",
                    dist.Normal(0, 1)
                    .expand([model_config.vae_latent_dim])
                    .to_event(1),
                )

                # Use decoder to generate r parameters from latent space
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Define base distribution with VAE-generated r
                base_dist = dist.NegativeBinomialProbs(r, p).to_event(1)
                numpyro.sample("counts", base_dist, obs=counts)
        else:
            # With batching: sample counts for a subset of cells
            with numpyro.plate(
                "cells", n_cells, subsample_size=batch_size
            ) as idx:
                # Sample from latent space prior
                z = numpyro.sample(
                    "z",
                    dist.Normal(0, 1)
                    .expand([model_config.vae_latent_dim])
                    .to_event(1),
                )

                # Use decoder to generate r parameters from latent space
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Define base distribution with VAE-generated r
                base_dist = dist.NegativeBinomialProbs(r, p).to_event(1)
                numpyro.sample("counts", base_dist, obs=counts[idx])
    else:
        # Without counts: for prior predictive sampling
        with numpyro.plate("cells", n_cells):
            # Sample from latent space prior
            z = numpyro.sample(
                "z",
                dist.Normal(0, 1)
                .expand([model_config.vae_latent_dim])
                .to_event(1),
            )

            # Use decoder to generate r parameters from latent space
            log_r = numpyro.deterministic("log_r", decoder_module(z))
            r = numpyro.deterministic("r", jnp.exp(log_r))

            # Define base distribution with VAE-generated r
            base_dist = dist.NegativeBinomialProbs(r, p).to_event(1)
            numpyro.sample("counts", base_dist)


# ------------------------------------------------------------------------------
# NBDM VAE Guide
# ------------------------------------------------------------------------------


def nbdm_vae_guide(
    n_cells: int,
    n_genes: int,
    model_config: ModelConfig,
    encoder: Encoder,  # Pre-created encoder passed as argument
    counts=None,
    batch_size=None,
):
    """
    Mean-field variational guide for the VAE-based NBDM model with standard
    parameterization.

    This guide defines a variational approximation for the VAE-based NBDM model,
    which combines the benefits of variational inference with the expressive
    power of deep neural networks. The guide approximates the posterior
    distribution of the model parameters using a mean-field factorization.

    The variational family consists of:
        1. A global parameter p with learnable alpha and beta parameters
        2. Cell-specific latent variables z with encoder-derived parameters
        3. The encoder network that maps observed counts to latent space
           parameters

    The guide architecture:
        - p ~ Beta(p_alpha, p_beta) where p_alpha and p_beta are learnable
          parameters with positivity constraints
        - z[cell] ~ Normal(z_mean[cell], z_std[cell]) where z_mean and z_std are
          computed by the encoder network from the observed counts

    The encoder network takes the observed gene expression counts for each cell
    and outputs the mean and log-variance of the approximate posterior for the
    latent variable z. This allows the model to learn a meaningful
    representation of the data in the latent space while maintaining the
    interpretability of the global parameter p.

    The VAE architecture provides several advantages:
        - Efficient learning of complex, non-linear relationships in the data
        - Automatic feature extraction and dimensionality reduction
        - Scalable inference through amortized variational inference
        - Meaningful latent representations for downstream analysis

    The guide supports optional batching over cells for scalable inference. When
    batching is enabled, the encoder processes only a subset of cells at a time,
    making it suitable for large datasets.

    Parameters
    ----------
    n_cells : int
        Number of cells (samples) in the dataset.
    n_genes : int
        Number of genes (features) in the dataset.
    model_config : ModelConfig
        Configuration object containing prior and guide parameter settings.
    encoder : Encoder
        Pre-trained encoder network that maps observed counts to latent space
        parameters.
    counts : Optional[jnp.ndarray], default=None
        Observed count data used by the encoder to compute latent space
        parameters.
    batch_size : Optional[int], default=None
        If specified, enables subsampling of cells for stochastic variational
        inference.

    Returns
    -------
    None
        This function defines the variational guide for use with NumPyro's
        inference machinery.
    """
    # Define guide parameters for p
    p_prior_params = model_config.p_param_prior or (1.0, 1.0)

    # Register p_alpha as a variational parameter with positivity constraint
    p_alpha = numpyro.param(
        "p_alpha", p_prior_params[0], constraint=constraints.positive
    )
    # Register p_beta as a variational parameter with positivity constraint
    p_beta = numpyro.param(
        "p_beta", p_prior_params[1], constraint=constraints.positive
    )
    # Sample p from the Beta distribution parameterized by p_alpha and p_beta
    numpyro.sample("p", dist.Beta(p_alpha, p_beta))

    # Register the pre-created encoder as NumPyro module for the guide
    encoder_module = nnx_module("encoder", encoder)

    # Sample latent variables using encoder
    if counts is not None:
        if batch_size is None:
            # Without batching: sample latent variables for all cells
            with numpyro.plate("cells", n_cells):
                # Use encoder to get mean and log variance for latent space
                z_mean, z_logvar = encoder_module(counts)
                z_std = jnp.exp(0.5 * z_logvar)

                # Sample from variational distribution
                numpyro.sample("z", dist.Normal(z_mean, z_std).to_event(1))
        else:
            # With batching: sample latent variables for a subset of cells
            with numpyro.plate(
                "cells", n_cells, subsample_size=batch_size
            ) as idx:
                # Use encoder to get mean and log variance for latent space
                batch_data = counts[idx]
                z_mean, z_logvar = encoder_module(batch_data)
                z_std = jnp.exp(0.5 * z_logvar)

                # Sample from variational distribution
                numpyro.sample("z", dist.Normal(z_mean, z_std).to_event(1))
    else:
        # Without counts: for prior predictive sampling
        with numpyro.plate("cells", n_cells):
            # Sample from latent space prior
            numpyro.sample(
                "z",
                dist.Normal(0, 1)
                .expand([model_config.vae_latent_dim])
                .to_event(1),
            )


# ------------------------------------------------------------------------------
# NBVCP VAE Model
# ------------------------------------------------------------------------------


def nbvcp_vae_model(
    n_cells: int,
    n_genes: int,
    model_config: ModelConfig,
    decoder: Decoder,  # Pre-created decoder passed as argument
    counts=None,
    batch_size=None,
):
    """
    Implements the VAE-based Negative Binomial model with variable mRNA capture
    probability (NBVCP) using standard parameterization suitable for variational
    inference in single-cell RNA sequencing data.

    This model extends the standard NBVCP model by incorporating a Variational
    Autoencoder (VAE) architecture. The VAE generates cell-specific dispersion
    parameters (r) from a low-dimensional latent space, while maintaining the
    global success probability parameter (p) and cell-specific capture
    probabilities (p_capture) as interpretable parameters.

    The model architecture consists of:
        1. A global parameter p sampled from a Beta prior distribution
        2. A latent variable z sampled from a standard Normal prior
        3. A decoder network that maps z to cell-specific r parameters
        4. Cell-specific capture probabilities p_capture that modify the
           effective success probability
        5. The Negative Binomial likelihood using VAE-generated r and modified p

    The model introduces a cell-specific mRNA capture probability p_capture ∈
    (0, 1), which modifies the effective success probability for each cell and
    gene. The effective success probability for cell i and gene j is:

        p_hat[i, j] = p * p_capture[i] / (1 - p * (1 - p_capture[i]))

    For each cell, the observed counts vector (of length n_genes) is modeled as:

        counts[cell, :] ~ NegativeBinomialProbs(r[cell, :], p_hat[cell, :])

    where r[cell, :] is generated by the VAE decoder from the latent variable z,
    and p_hat[cell, :] incorporates both the global success probability p and
    the cell-specific capture probability p_capture.

    The VAE architecture allows the model to:
        - Capture cell-to-cell heterogeneity in dispersion parameters
        - Model technical variation through cell-specific capture probabilities
        - Learn meaningful low-dimensional representations of the data
        - Maintain interpretability through global and cell-specific parameters
        - Scale to large datasets through efficient latent space modeling

    The model supports optional batching over cells for scalable inference. If
    `counts` is provided, it is used as observed data; otherwise, the model
    samples counts from the generative process.

    Parameters
    ----------
    n_cells : int
        Number of cells in the dataset.
    n_genes : int
        Number of genes (features) per cell.
    model_config : ModelConfig
        ModelConfig object specifying prior parameters and VAE configuration.
    decoder : Decoder
        Pre-trained decoder network that maps latent variables to r parameters.
    counts : Optional[jnp.ndarray], default=None
        Observed count data (not used in the model, but included for interface
        compatibility).
    batch_size : Optional[int], default=None
        If specified, enables subsampling of cells for stochastic variational
        inference.

    Returns
    -------
    None
        This function defines the probabilistic model for use with NumPyro.
    """
    # Define prior parameters
    p_prior_params = model_config.p_param_prior or (1.0, 1.0)
    p_capture_prior_params = model_config.p_capture_param_prior or (1.0, 1.0)

    # Sample global success probability p from Beta prior
    p = numpyro.sample("p", dist.Beta(*p_prior_params))

    # Register the pre-created decoder as NumPyro module
    decoder_module = nnx_module("decoder", decoder)

    # Sample counts
    if counts is not None:
        if batch_size is None:
            # Without batching: sample counts for all cells
            with numpyro.plate("cells", n_cells):
                # Sample from latent space prior
                z = numpyro.sample(
                    "z",
                    dist.Normal(0, 1)
                    .expand([model_config.vae_latent_dim])
                    .to_event(1),
                )

                # Use decoder to generate r parameters from latent space
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Sample cell-specific capture probability from Beta prior
                p_capture = numpyro.sample(
                    "p_capture", dist.Beta(*p_capture_prior_params)
                )
                # Reshape p_capture for broadcasting to (n_cells, n_genes)
                p_capture = p_capture[:, None]  # Shape: (batch_size, 1)
                # Compute effective success probability p_hat for each cell/gene
                p_hat = p * p_capture / (1 - p * (1 - p_capture))

                # Define base distribution with VAE-generated r
                base_dist = dist.NegativeBinomialProbs(r, p_hat).to_event(1)
                numpyro.sample("counts", base_dist, obs=counts)
        else:
            # With batching: sample counts for a subset of cells
            with numpyro.plate(
                "cells", n_cells, subsample_size=batch_size
            ) as idx:
                # Sample from latent space prior
                z = numpyro.sample(
                    "z",
                    dist.Normal(0, 1)
                    .expand([model_config.vae_latent_dim])
                    .to_event(1),
                )

                # Use decoder to generate r parameters from latent space
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Sample cell-specific capture probability from Beta prior
                p_capture = numpyro.sample(
                    "p_capture", dist.Beta(*p_capture_prior_params)
                )
                # Reshape p_capture for broadcasting to (n_cells, n_genes)
                p_capture = p_capture[:, None]  # Shape: (batch_size, 1)
                # Compute effective success probability p_hat for each cell/gene
                p_hat = p * p_capture / (1 - p * (1 - p_capture))

                # Define base distribution with VAE-generated r
                base_dist = dist.NegativeBinomialProbs(r, p_hat).to_event(1)
                numpyro.sample("counts", base_dist, obs=counts[idx])
    else:
        # Without counts: for prior predictive sampling
        with numpyro.plate("cells", n_cells):
            # Sample from latent space prior
            z = numpyro.sample(
                "z",
                dist.Normal(0, 1)
                .expand([model_config.vae_latent_dim])
                .to_event(1),
            )

            # Use decoder to generate r parameters from latent space
            log_r = numpyro.deterministic("log_r", decoder_module(z))
            r = numpyro.deterministic("r", jnp.exp(log_r))

            # Sample cell-specific capture probability from Beta prior
            p_capture = numpyro.sample(
                "p_capture", dist.Beta(*p_capture_prior_params)
            )
            # Reshape p_capture for broadcasting to (n_cells, n_genes)
            p_capture = p_capture[:, None]  # Shape: (batch_size, 1)
            # Compute effective success probability p_hat for each cell/gene
            p_hat = p * p_capture / (1 - p * (1 - p_capture))

            # Define base distribution with VAE-generated r
            base_dist = dist.NegativeBinomialProbs(r, p_hat).to_event(1)
            numpyro.sample("counts", base_dist)


# ------------------------------------------------------------------------------
# NBVCP VAE Guide
# ------------------------------------------------------------------------------


def nbvcp_vae_guide(
    n_cells: int,
    n_genes: int,
    model_config: ModelConfig,
    encoder: EncoderVCP,  # Pre-created encoder passed as argument
    counts=None,
    batch_size=None,
):
    """
    Mean-field variational guide for the VAE-based NBVCP model with standard
    parameterization.

    This guide defines a variational approximation for the VAE-based NBVCP
    model, which combines the benefits of variational inference with the
    expressive power of deep neural networks and cell-specific capture
    probabilities. The guide approximates the posterior distribution of the
    model parameters using a mean-field factorization.

    The variational family consists of:
        1. A global parameter p with learnable alpha and beta parameters
        2. Cell-specific latent variables z with encoder-derived parameters
        3. Cell-specific capture probability parameters p_capture with
           encoder-derived parameters
        4. The encoder network that maps observed counts to latent space and
           capture parameters

    The guide architecture:
        - p ~ Beta(p_alpha, p_beta) where p_alpha and p_beta are learnable
          parameters with positivity constraints
        - z[cell] ~ Normal(z_mean[cell], z_std[cell]) where z_mean and z_std are
          computed by the encoder network from the observed counts
        - p_capture[cell] ~ Beta(p_capture_alpha[cell], p_capture_beta[cell])
          where the parameters are computed by the encoder network

    The encoder network takes the observed gene expression counts for each cell
    and outputs:
        1. The mean and log-variance of the approximate posterior for the latent
           variable z
        2. The alpha and beta parameters for the cell-specific capture
           probabilities

    This allows the model to learn meaningful representations of the data in the
    latent space while maintaining the interpretability of the global parameter
    p and capturing cell-specific technical effects through p_capture.

    The VAE architecture provides several advantages:
        - Efficient learning of complex, non-linear relationships in the data
        - Automatic feature extraction and dimensionality reduction
        - Scalable inference through amortized variational inference
        - Meaningful latent representations for downstream analysis
        - Modeling of technical variation through capture probabilities

    The guide supports optional batching over cells for scalable inference. When
    batching is enabled, the encoder processes only a subset of cells at a time,
    making it suitable for large datasets.

    Parameters
    ----------
    n_cells : int
        Number of cells (samples) in the dataset.
    n_genes : int
        Number of genes (features) in the dataset.
    model_config : ModelConfig
        Configuration object containing prior and guide parameter settings.
    encoder : EncoderVCP
        Pre-trained encoder network that maps observed counts to latent space
        parameters and capture probability parameters.
    counts : Optional[jnp.ndarray], default=None
        Observed count data used by the encoder to compute latent space and
        capture probability parameters.
    batch_size : Optional[int], default=None
        If specified, enables subsampling of cells for stochastic variational
        inference.

    Returns
    -------
    None
        This function defines the variational guide for use with NumPyro's
        inference machinery.
    """
    # Define guide parameters for p
    p_prior_params = model_config.p_param_guide or (1.0, 1.0)
    p_capture_prior_params = model_config.p_capture_param_guide or (1.0, 1.0)

    # Register p_alpha as a variational parameter with positivity constraint
    p_alpha = numpyro.param(
        "p_alpha", p_prior_params[0], constraint=constraints.positive
    )
    # Register p_beta as a variational parameter with positivity constraint
    p_beta = numpyro.param(
        "p_beta", p_prior_params[1], constraint=constraints.positive
    )
    # Sample p from the Beta distribution parameterized by p_alpha and p_beta
    numpyro.sample("p", dist.Beta(p_alpha, p_beta))

    # Register the pre-created encoder as NumPyro module for the guide
    encoder_module = nnx_module("encoder", encoder)

    # Sample latent variables using encoder
    if counts is not None:
        if batch_size is None:
            # Without batching: sample latent variables for all cells
            with numpyro.plate("cells", n_cells):
                # Use encoder to get mean and log variance for latent space
                z_mean, z_logvar, p_logalpha, p_logbeta = encoder_module(counts)
                z_std = jnp.exp(0.5 * z_logvar)

                # Sample from variational distribution
                numpyro.sample("z", dist.Normal(z_mean, z_std).to_event(1))

                # Sample cell-specific capture probability from Beta prior
                numpyro.sample(
                    "p_capture",
                    dist.Beta(
                        jnp.exp(p_logalpha.squeeze(-1)),
                        jnp.exp(p_logbeta.squeeze(-1)),
                    ),
                )
        else:
            # With batching: sample latent variables for a subset of cells
            with numpyro.plate(
                "cells", n_cells, subsample_size=batch_size
            ) as idx:
                # Use encoder to get mean and log variance for latent space
                batch_data = counts[idx]
                z_mean, z_logvar, p_logalpha, p_logbeta = encoder_module(
                    batch_data
                )
                z_std = jnp.exp(0.5 * z_logvar)

                # Sample from variational distribution
                numpyro.sample("z", dist.Normal(z_mean, z_std).to_event(1))

                # Sample cell-specific capture probability from Beta prior
                numpyro.sample(
                    "p_capture",
                    dist.Beta(
                        jnp.exp(p_logalpha.squeeze(-1)),
                        jnp.exp(p_logbeta.squeeze(-1)),
                    ),
                )
    else:
        # Without counts: for prior predictive sampling
        with numpyro.plate("cells", n_cells):
            # Sample from latent space prior
            numpyro.sample(
                "z",
                dist.Normal(0, 1)
                .expand([model_config.vae_latent_dim])
                .to_event(1),
            )

            # Sample cell-specific capture probability from Beta prior
            numpyro.sample("p_capture", dist.Beta(*p_capture_prior_params))


# ==============================================================================
# dpVAE Model Functions (for decoupled prior)
# ==============================================================================

# ------------------------------------------------------------------------------
# NBDM dpVAE Model
# ------------------------------------------------------------------------------


def nbdm_dpvae_model(
    n_cells: int,
    n_genes: int,
    model_config: ModelConfig,
    decoder: Decoder,
    decoupled_prior: DecoupledPrior,
    counts=None,
    batch_size=None,
):
    """
    Implements the decoupled prior VAE (dpVAE) model for Negative
    Binomial-Dirichlet Multinomial (NBDM) data using standard parameterization.

    This model extends the standard VAE-based NBDM model by incorporating a
    decoupled prior architecture, which allows for more flexible and expressive
    prior distributions over the latent space. The key innovation is the use of
    a DecoupledPriorDistribution that combines a base distribution with a
    learned prior network.

    The model architecture consists of:
        1. A global parameter p sampled from a Beta prior distribution
        2. A latent variable z sampled from a decoupled prior distribution
        3. A decoder network that maps z to cell-specific r parameters
        4. The Negative Binomial likelihood using VAE-generated r and global p
        5. A decoupled prior network that learns flexible prior distributions

    The decoupled prior combines a standard Normal base distribution with a
    learned prior network that can adapt to the structure of the data, providing
    more expressive modeling capabilities than standard VAEs.

    For each cell, the observed counts vector (of length n_genes) is modeled as:

        counts[cell, :] ~ NegativeBinomialProbs(r[cell, :], p)

    where r[cell, :] is generated by the VAE decoder from the latent variable z,
    and p is the global success probability shared across all cells.

    The decoupled prior architecture provides several advantages:
        - More flexible and expressive prior distributions
        - Better adaptation to the structure of the data
        - Improved posterior approximation quality
        - Enhanced latent space representations
        - Better handling of complex data distributions

    The model supports optional batching over cells for scalable inference. If
    `counts` is provided, it is used as observed data; otherwise, the model
    samples counts from the generative process.

    Parameters
    ----------
    n_cells : int
        Number of cells in the dataset.
    n_genes : int
        Number of genes (features) per cell.
    model_config : ModelConfig
        ModelConfig object specifying prior parameters and VAE configuration.
    decoder : Decoder
        Pre-trained decoder network that maps latent variables to r parameters.
    decoupled_prior : DecoupledPrior
        Pre-trained decoupled prior network that learns flexible prior
        distributions.
    counts : Optional[jnp.ndarray], default=None
        Observed count data (not used in the model, but included for interface
        compatibility).
    batch_size : Optional[int], default=None
        If specified, enables subsampling of cells for stochastic variational
        inference.

    Returns
    -------
    None
        This function defines the probabilistic model for use with NumPyro.
    """
    # Define prior parameters
    p_prior_params = model_config.p_param_prior or (1.0, 1.0)

    # Sample global parameters
    p = numpyro.sample("p", dist.Beta(*p_prior_params))

    # Register the decoder and decoupled prior as NumPyro modules
    decoder_module = nnx_module("decoder", decoder)
    decoupled_prior_module = nnx_module("decoupled_prior", decoupled_prior)

    # Create the decoupled prior distribution
    base_distribution = dist.Normal(
        jnp.zeros(model_config.vae_latent_dim),
        jnp.ones(model_config.vae_latent_dim),
    ).to_event(1)
    decoupled_prior_dist = DecoupledPriorDistribution(
        decoupled_prior=decoupled_prior_module,
        base_distribution=base_distribution,
    )

    # Sample latent variables and generate observations
    if counts is not None:
        if batch_size is None:
            with numpyro.plate("cells", n_cells):
                # Sample z from decoupled prior (KEY DIFFERENCE from standard
                # VAE)
                z = numpyro.sample("z", decoupled_prior_dist)

                # Decode z to get r parameters
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Define base distribution with VAE-generated r
                base_dist = dist.NegativeBinomialProbs(r, p).to_event(1)
                numpyro.sample("counts", base_dist, obs=counts)
        else:
            with numpyro.plate(
                "cells", n_cells, subsample_size=batch_size
            ) as idx:
                # Sample z from decoupled prior
                z = numpyro.sample("z", decoupled_prior_dist)

                # Decode z to get r parameters
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Sample observed counts
                batch_counts = counts[idx] if counts is not None else None
                numpyro.sample(
                    "counts",
                    dist.NegativeBinomialProbs(r, p).to_event(1),
                    obs=batch_counts,
                )
    else:
        # Without counts: for prior predictive sampling
        with numpyro.plate("cells", n_cells):
            # Sample from latent space prior
            z = numpyro.sample("z", decoupled_prior_dist)

            # Use decoder to generate r parameters from latent space
            log_r = numpyro.deterministic("log_r", decoder_module(z))
            r = numpyro.deterministic("r", jnp.exp(log_r))

            # Define base distribution with VAE-generated r
            base_dist = dist.NegativeBinomialProbs(r, p).to_event(1)
            numpyro.sample("counts", base_dist)


# ------------------------------------------------------------------------------
# NBVCP dpVAE Model
# ------------------------------------------------------------------------------


def nbvcp_dpvae_model(
    n_cells: int,
    n_genes: int,
    model_config: ModelConfig,
    decoder: Decoder,
    decoupled_prior: DecoupledPrior,
    counts=None,
    batch_size=None,
):
    """
    Implements the decoupled prior VAE (dpVAE) model for Negative Binomial with
    variable mRNA capture probability (NBVCP) using standard parameterization.

    This model extends the standard VAE-based NBVCP model by incorporating a
    decoupled prior architecture, which allows for more flexible and expressive
    prior distributions over the latent space. The key innovation is the use of
    a DecoupledPriorDistribution that combines a base distribution with a
    learned prior network.

    The model architecture consists of:
        1. A global parameter p sampled from a Beta prior distribution
        2. A latent variable z sampled from a decoupled prior distribution
        3. A decoder network that maps z to cell-specific r parameters
        4. Cell-specific capture probabilities p_capture that modify the
           effective success probability
        5. The Negative Binomial likelihood using VAE-generated r and modified p
        6. A decoupled prior network that learns flexible prior distributions

    The model introduces a cell-specific mRNA capture probability p_capture ∈
    (0, 1), which modifies the effective success probability for each cell and
    gene. The effective success probability for cell i and gene j is:

        p_hat[i, j] = p * p_capture[i] / (1 - p * (1 - p_capture[i]))

    For each cell, the observed counts vector (of length n_genes) is modeled as:

        counts[cell, :] ~ NegativeBinomialProbs(r[cell, :], p_hat[cell, :])

    where r[cell, :] is generated by the VAE decoder from the latent variable z,
    and p_hat[cell, :] incorporates both the global success probability p and
    the cell-specific capture probability p_capture.

    The decoupled prior combines a standard Normal base distribution with a
    learned prior network that can adapt to the structure of the data, providing
    more expressive modeling capabilities than standard VAEs.

    The decoupled prior architecture provides several advantages:
        - More flexible and expressive prior distributions
        - Better adaptation to the structure of the data
        - Improved posterior approximation quality
        - Enhanced latent space representations
        - Better handling of complex data distributions
        - Modeling of technical variation through capture probabilities

    The model supports optional batching over cells for scalable inference. If
    `counts` is provided, it is used as observed data; otherwise, the model
    samples counts from the generative process.

    Parameters
    ----------
    n_cells : int
        Number of cells in the dataset.
    n_genes : int
        Number of genes (features) per cell.
    model_config : ModelConfig
        ModelConfig object specifying prior parameters and VAE configuration.
    decoder : Decoder
        Pre-trained decoder network that maps latent variables to r parameters.
    decoupled_prior : DecoupledPrior
        Pre-trained decoupled prior network that learns flexible prior
        distributions.
    counts : Optional[jnp.ndarray], default=None
        Observed count data (not used in the model, but included for interface
        compatibility).
    batch_size : Optional[int], default=None
        If specified, enables subsampling of cells for stochastic variational
        inference.

    Returns
    -------
    None
        This function defines the probabilistic model for use with NumPyro.
    """
    # Define prior parameters
    p_prior_params = model_config.p_param_prior or (1.0, 1.0)
    p_capture_prior_params = model_config.p_capture_param_prior or (1.0, 1.0)

    # Sample global success probability p from Beta prior
    p = numpyro.sample("p", dist.Beta(*p_prior_params))

    # Register the decoder and decoupled prior as NumPyro modules
    decoder_module = nnx_module("decoder", decoder)
    decoupled_prior_module = nnx_module("decoupled_prior", decoupled_prior)

    # Create the decoupled prior distribution
    base_distribution = dist.Normal(
        jnp.zeros(model_config.vae_latent_dim),
        jnp.ones(model_config.vae_latent_dim),
    ).to_event(1)
    decoupled_prior_dist = DecoupledPriorDistribution(
        decoupled_prior=decoupled_prior_module,
        base_distribution=base_distribution,
    )

    # Sample counts
    if counts is not None:
        if batch_size is None:
            # Without batching: sample counts for all cells
            with numpyro.plate("cells", n_cells):
                # Sample z from decoupled prior (KEY DIFFERENCE from standard
                # VAE)
                z = numpyro.sample("z", decoupled_prior_dist)

                # Use decoder to generate r parameters from latent space
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Sample cell-specific capture probability from Beta prior
                p_capture = numpyro.sample(
                    "p_capture", dist.Beta(*p_capture_prior_params)
                )
                # Reshape p_capture for broadcasting to (n_cells, n_genes)
                p_capture = p_capture[:, None]
                # Compute effective success probability p_hat for each cell/gene
                p_hat = p * p_capture / (1 - p * (1 - p_capture))

                # Define base distribution with VAE-generated r
                base_dist = dist.NegativeBinomialProbs(r, p_hat).to_event(1)
                numpyro.sample("counts", base_dist, obs=counts)
        else:
            # With batching: sample counts for a subset of cells
            with numpyro.plate(
                "cells", n_cells, subsample_size=batch_size
            ) as idx:
                # Sample z from decoupled prior (KEY DIFFERENCE from standard
                # VAE)
                z = numpyro.sample("z", decoupled_prior_dist)

                # Use decoder to generate r parameters from latent space
                log_r = numpyro.deterministic("log_r", decoder_module(z))
                r = numpyro.deterministic("r", jnp.exp(log_r))

                # Sample cell-specific capture probability from Beta prior
                p_capture = numpyro.sample(
                    "p_capture", dist.Beta(*p_capture_prior_params)
                )
                # Reshape p_capture for broadcasting to (n_cells, n_genes)
                p_capture = p_capture[:, None]  # Shape: (batch_size, 1)
                # Compute effective success probability p_hat for each cell/gene
                p_hat = p * p_capture / (1 - p * (1 - p_capture))

                # Define base distribution with VAE-generated r
                base_dist = dist.NegativeBinomialProbs(r, p_hat).to_event(1)
                numpyro.sample("counts", base_dist, obs=counts[idx])
    else:
        # Without counts: for prior predictive sampling
        with numpyro.plate("cells", n_cells):
            # Sample z from decoupled prior (KEY DIFFERENCE from standard VAE)
            z = numpyro.sample("z", decoupled_prior_dist)

            # Use decoder to generate r parameters from latent space
            log_r = numpyro.deterministic("log_r", decoder_module(z))
            r = numpyro.deterministic("r", jnp.exp(log_r))

            # Sample cell-specific capture probability from Beta prior
            p_capture = numpyro.sample(
                "p_capture", dist.Beta(*p_capture_prior_params)
            )
            # Reshape p_capture for broadcasting to (n_cells, n_genes)
            p_capture = p_capture[:, None]  # Shape: (batch_size, 1)
            # Compute effective success probability p_hat for each cell/gene
            p_hat = p * p_capture / (1 - p * (1 - p_capture))
            p_hat = p_hat[:, None]  # Shape: (batch_size, 1)
            p_hat = p_hat * jnp.ones(
                (1, n_genes)
            )  # Shape: (batch_size, n_genes)

            # Define base distribution with VAE-generated r
            base_dist = dist.NegativeBinomialProbs(r, p_hat).to_event(1)
            numpyro.sample("counts", base_dist)


# ==============================================================================
# Posterior Distribution Functions
# ==============================================================================


def get_posterior_distributions(
    params: Dict[str, jnp.ndarray],
    model_config: ModelConfig,
    vae_model: VAE,
) -> Dict[str, dist.Distribution]:
    """
    Construct posterior distributions for model parameters from variational
    guide outputs for VAE-based models with standard parameterization.

    This function builds the appropriate `numpyro` distributions based on the
    guide parameters found in the `params` dictionary for VAE-based models. It
    handles both standard VAE models and decoupled prior VAE (dpVAE) models.

    The function constructs posterior distributions for:
        - Global parameters (p with alpha/beta parameters)
        - VAE-specific parameters (latent variables z)
        - Optional parameters (p_capture with alpha/beta parameters)

    For VAE models, the latent variable z is obtained from the VAE model's prior
    distribution, which may be either a standard Normal distribution or a
    learned decoupled prior distribution.

    The function is designed to work with the standard parameterization where
    parameters are sampled from appropriate prior distributions (e.g., Beta for
    p, Normal for z) and transformed to their constrained spaces as needed.

    Parameters
    ----------
    params : Dict[str, jnp.ndarray]
        Dictionary containing estimated variational parameters for each latent
        variable, as produced by the guide. Expected keys include:
            - "p_alpha", "p_beta" for global success probability
            - "p_capture_alpha", "p_capture_beta" for capture probabilities
        Each value is a JAX array of appropriate shape (scalar or vector).
    model_config : ModelConfig
        Model configuration object containing VAE-specific settings.
    vae_model : VAE
        The VAE model object that provides the prior distribution for latent
        variables z.

    Returns
    -------
    Dict[str, dist.Distribution]
        Dictionary mapping parameter names to their corresponding posterior
        distributions. All distributions are returned as batch distributions (no
        splitting or per-component/gene lists).
    """
    distributions = {}

    # p parameter (Beta distribution)
    if "p_alpha" in params and "p_beta" in params:
        distributions["p"] = dist.Beta(params["p_alpha"], params["p_beta"])

    # p_capture parameter (Beta distribution)
    if "p_capture_alpha" in params and "p_capture_beta" in params:
        distributions["p_capture"] = dist.Beta(
            params["p_capture_alpha"], params["p_capture_beta"]
        )

    # Get the decoupled prior distribution
    distributions["z"] = vae_model.get_prior_distribution()

    return distributions
