{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import time\n",
    "import utils\n",
    "import emcee\n",
    "import numpy as np\n",
    "import scipy.special\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import multiprocessing as mp\n",
    "from tqdm import tqdm\n",
    "\n",
    "cor, pal = utils.matplotlib_style()\n",
    "\n",
    "\n",
    "# Set number of threads for other non-pool processes to one to avoid conflicts\n",
    "os.environ[\"OMP_NUM_THREADS\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The two-state negative binomial approximation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will numerically explore the two-state promoter model as it\n",
    "approximates the negative binomial distribution. As we have described before,\n",
    "the steady-state mRNA/UMI distribution for a two-state promoter is given by\n",
    "$$\n",
    "{\\scriptstyle\n",
    "\\pi(u \\mid \\hat{\\theta}) = \n",
    "\\frac{1}{\\Gamma(u + 1)}\n",
    "\\frac{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\hat{k}^{(p)}_{\\text{on}} + u\n",
    "    \\right)\n",
    "}{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\hat{k}^{(p)}_{\\text{on}}\n",
    "    \\right)\n",
    "}\n",
    "\\frac{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\hat{k}^{(p)}_{\\text{on}} + \\hat{k}^{(p)}_{\\text{off}}\n",
    "    \\right)\n",
    "}{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\hat{k}^{(p)}_{\\text{on}} + \\hat{k}^{(p)}_{\\text{off}} + u\n",
    "    \\right)\n",
    "}\n",
    "\\left( \\hat{r}_u \\right)^u \\\\\n",
    "\\times {}_1F_1 \n",
    "\\left(\n",
    "    \\hat{k}^{(p)}_{\\text{on}} + u,\n",
    "    \\hat{k}^{(p)}_{\\text{on}} + \\hat{k}^{(p)}_{\\text{off}} + u,\n",
    "    - \\hat{r}_u\n",
    "\\right),\n",
    "}\n",
    "\\tag{1}\n",
    "$$\n",
    "where $\\hat{\\theta} = \\left\\{ \\hat{k}^{(p)}_{\\text{on}},\n",
    "\\hat{k}^{(p)}_{\\text{off}}, \\hat{r}_u \\right\\}$ are the parameters of the model\n",
    "rescaled by the mRNA degradation rate $\\gamma_m$, i.e.,\n",
    "$$\n",
    "\\hat{x} = \\frac{x}{\\gamma_m}.\n",
    "\\tag{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have also shown that in the regime $\\hat{k}^{(p)}_{\\text{off}} \\gg 1$, Eq.\n",
    "(1) can be provably rewritten as\n",
    "$$\n",
    "\\pi(u \\mid \\hat{\\theta}) =\n",
    "{\n",
    "u + \\hat{k}^{(p)}_{\\text{on}} - 1\n",
    "\\choose\n",
    "u\n",
    "}\n",
    "\\left( \n",
    "    \\frac{\n",
    "        \\frac{\\hat{r}_u}{\\hat{k}^{(p)}_{\\text{off}}} \n",
    "    }{\n",
    "        1 + \\frac{\\hat{r}_u}{\\hat{k}^{(p)}_{\\text{off}}}\n",
    "    }\n",
    "\\right)^m\n",
    "\\left(\n",
    "    \\frac{\n",
    "        1\n",
    "    }{\n",
    "        1 + \\frac{\\hat{r}_u}{\\hat{k}^{(p)}_{\\text{off}}}\n",
    "    }\n",
    "\\right)^{\\hat{k}^{(p)}_{\\text{on}}},\n",
    "\\tag{3}\n",
    "$$\n",
    "i.e., the negative binomial PMF. The numerical question is then at what values\n",
    "of $\\hat{k}^{(p)}_{\\text{off}}$ does Eq. (3) become a good approximation of Eq.\n",
    "(1)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To answer this question, we will numerically explore the two-state promoter\n",
    "model for different parameter values. We already implemented the two-state\n",
    "promoter model, including the integral approximation of the confluent\n",
    "hypergeometric function in our `two_state_log_probability` function. Let's write\n",
    "the equivalent function for Eq. (3)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def two_state_neg_binom_log_probability(\n",
    "    mRNA_values,\n",
    "    kp_on,\n",
    "    kp_off,\n",
    "    rm,\n",
    "    gm=1,\n",
    "):\n",
    "    \"\"\"\n",
    "    Compute the log probability of the negative binomial approximation of the\n",
    "    two-state promoter model.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    mRNA_values : array-like\n",
    "        The range of mRNA values to evaluate.\n",
    "    kp_on : float\n",
    "        Rate of activation of the promoter.\n",
    "    kp_off : float\n",
    "        Rate of deactivation of the promoter.\n",
    "    rm : float\n",
    "        Production rate of the mRNA.\n",
    "    gm : float, optional\n",
    "        1 / half-life time for the mRNA. Default is 1.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    numpy.ndarray\n",
    "        The computed log probabilities for the range of mRNA values.\n",
    "\n",
    "    Notes\n",
    "    -----\n",
    "    This function uses the negative binomial approximation of the two-state\n",
    "    promoter model, which is valid when kp_off >> 1.\n",
    "    \"\"\"\n",
    "    # Define the parameters\n",
    "    n = mRNA_values\n",
    "    k = kp_on / gm\n",
    "    p = 1 / (1 + rm / kp_off)\n",
    "    # Compute the log probability using scipy.stats.nbinom\n",
    "    log_prob = scipy.stats.nbinom.logpmf(n, k, p)\n",
    "    return log_prob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now test this function by comparing the two-state promoter model and the\n",
    "negative binomial approximation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k_on = 4.0\n",
    "k_off = 18.0\n",
    "r_u = 100.0\n",
    "\n",
    "# Define range of UMI counts\n",
    "u_range = np.arange(0, 75)\n",
    "\n",
    "# Evaluate the log probability\n",
    "logP_two_state = utils.two_state_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "logP_neg_binom = two_state_neg_binom_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Plot the probability\n",
    "ax.step(u_range, np.exp(logP_two_state), label=\"Two-state promoter\")\n",
    "ax.step(u_range, np.exp(logP_neg_binom), label=\"Negative binomial\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"UMI counts\")\n",
    "ax.set_ylabel(\"probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this parameter regime the approximation is not very good. Let's see an \n",
    "example in the right parameter regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k_on = 10.0\n",
    "k_off = 200.0\n",
    "r_u = 2000.0\n",
    "\n",
    "# Define range of UMI counts\n",
    "u_range = np.arange(0, 400)\n",
    "\n",
    "# Evaluate the log probability\n",
    "logP_two_state = utils.two_state_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "logP_neg_binom = two_state_neg_binom_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Plot the probability\n",
    "ax.step(u_range, np.exp(logP_two_state), label=\"Two-state promoter\")\n",
    "ax.step(u_range, np.exp(logP_neg_binom), label=\"Negative binomial\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"UMI counts\")\n",
    "ax.set_ylabel(\"probability\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k_on = 10.0\n",
    "k_off = 200.0\n",
    "r_u = 20.0\n",
    "\n",
    "# Define range of UMI counts\n",
    "u_range = np.arange(0, 40)\n",
    "\n",
    "# Evaluate the log probability\n",
    "logP_two_state = utils.two_state_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "logP_neg_binom = two_state_neg_binom_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Plot the probability\n",
    "ax.step(u_range, np.exp(logP_two_state), label=\"Two-state promoter\")\n",
    "ax.step(u_range, np.exp(logP_neg_binom), label=\"Negative binomial\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"UMI counts\")\n",
    "ax.set_ylabel(\"probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this parameter regime the approximation is much better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k_on = 1\n",
    "k_off = 20\n",
    "r_u = 100.0\n",
    "\n",
    "# Define range of UMI counts\n",
    "u_range = np.arange(0, 4000)\n",
    "\n",
    "# Evaluate the log probability\n",
    "logP_two_state = utils.two_state_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "logP_neg_binom = two_state_neg_binom_log_probability(u_range, k_on, k_off, r_u)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Plot the probability\n",
    "ax.step(u_range, np.exp(logP_two_state), label=\"Two-state promoter\")\n",
    "ax.step(u_range, np.exp(logP_neg_binom), label=\"Negative binomial\")\n",
    "\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Add legend\n",
    "ax.legend(bbox_to_anchor=(1, 0.5))\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"UMI counts\")\n",
    "ax.set_ylabel(\"probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## KL divergence as a similarity measure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To systematically explore the parameter regime where the approximation is\n",
    "valid, we will compute the KL divergence between the two-state promoter model\n",
    "and the negative binomial approximation. We will use the KL divergence as a\n",
    "similarity measure between the two probability distributions. We can use the\n",
    "`scipy.stats.entropy` function to compute this quantity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define range of kp_on values\n",
    "kp_on_range = np.logspace(0, 2, 25)\n",
    "# Define range of r_u values\n",
    "r_u_range = np.logspace(0, 3, 25)\n",
    "\n",
    "# Define range of UMI counts\n",
    "u_range = np.arange(0, 2_000)\n",
    "\n",
    "# Define single kp_off value\n",
    "kp_off = 100.0\n",
    "\n",
    "# Initialize KL divergence matrix\n",
    "kl_divergence = np.zeros((len(kp_on_range), len(r_u_range)))\n",
    "\n",
    "# Count the number of iterations\n",
    "total_iterations = len(kp_on_range) * len(r_u_range)\n",
    "# Initialize progress bar\n",
    "with tqdm(total=total_iterations, desc=\"Computing KL divergence\") as pbar:\n",
    "    # Loop over all combinations of kp_on and r_u\n",
    "    for i, kp_on in enumerate(kp_on_range):\n",
    "        for j, r_u in enumerate(r_u_range):\n",
    "            # Compute the KL divergence\n",
    "            kl_divergence[i, j] = scipy.stats.entropy(\n",
    "                utils.two_state_log_probability(u_range, kp_on, kp_off, r_u),\n",
    "                qk=two_state_neg_binom_log_probability(\n",
    "                    u_range, kp_on, kp_off, r_u\n",
    "                ),\n",
    "            )\n",
    "            # Update progress bar\n",
    "            pbar.update(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the KL divergence matrix as a heatmap."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Plot the KL divergence matrix\n",
    "sns.heatmap(kl_divergence, ax=ax, cmap=\"viridis\")\n",
    "\n",
    "# Set x-axis ticks and labels\n",
    "ax.set_xticks([0, len(r_u_range)-1])\n",
    "ax.set_xticklabels([f\"{r_u_range[0]:.0f}\", f\"{r_u_range[-1]:.0f}\"])\n",
    "\n",
    "# Set y-axis ticks and labels\n",
    "ax.set_yticks([0, len(kp_on_range)-1])\n",
    "ax.set_yticklabels([f\"{kp_on_range[0]:.0f}\", f\"{kp_on_range[-1]:.0f}\"])\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(r\"$r_u$\")\n",
    "ax.set_ylabel(r\"$k^{(p)}_{on}$\")\n",
    "\n",
    "# Show figure\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrappy-jDg5b02t-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
