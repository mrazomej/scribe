{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import io\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "import mpmath\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from functools import partial\n",
    "import bayesflow as bf\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "# Set TensorFlow to use only the CPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "cor, pal = utils.matplotlib_style()\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-state promoter distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook, we will explore training a `BayesFlow` neural network to\n",
    "perform parametric inference on the biophysical parameters of a two-state\n",
    "promoter model. The two-state promoter model, also known as the \"telegraph\n",
    "model\", is a simple model of gene expression that describes the dynamics of a\n",
    "promotion region that can be in one of two states: active or inactive. The\n",
    "steady mRNA distribution can be obtained by solving the master equation for the\n",
    "system. The solution has a closed form and is given by:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "{\\scriptstyle\n",
    "P\\left(m \\mid k^{(p)}_{\\text{on}}, k^{(p)}_{\\text{off}}, r_m, \\gamma_m\\right) =\n",
    "\\frac{1}{\\Gamma(m + 1)}\n",
    "\\frac{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\frac{k^{(p)}_{\\text{on}}}{\\gamma_m} + m\n",
    "    \\right)\n",
    "}{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\frac{k^{(p)}_{\\text{on}}}{\\gamma_m}\n",
    "    \\right)\n",
    "}\n",
    "\\frac{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\frac{k^{(p)}_{\\text{on}} + k^{(p)}_{\\text{off}}}{\\gamma_m}\n",
    "    \\right)\n",
    "}{\n",
    "    \\Gamma\n",
    "    \\left(\n",
    "        \\frac{k^{(p)}_{\\text{on}} + k^{(p)}_{\\text{off}}}{\\gamma_m} + m \n",
    "    \\right)\n",
    "}\n",
    "\\left( \\frac{r_m}{\\gamma_m} \\right)^m \\\\\n",
    "\\times {}_1F_1 \n",
    "\\left(\n",
    "    \\frac{\n",
    "            k^{(p)}_{\\text{on}} \n",
    "        }{\n",
    "            \\gamma_m\n",
    "        }\n",
    "    + m,\n",
    "    \\frac{\n",
    "            k^{(p)}_{\\text{on}} + k^{(p)}_{\\text{off}} \n",
    "        }{\n",
    "            \\gamma_m\n",
    "        }\n",
    "    + m,\n",
    "    - \\frac{r_m}{\\gamma_m}\n",
    "\\right),\n",
    "}\n",
    "\\tag{1}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $m$ is the steady-state mRNA copy number per cell, $k_{\\text{on}}^{(p)}$\n",
    "and $k_{\\text{off}}^{(p)}$ are the rate constants for the promoter turning on\n",
    "and off, respectively, $r_m$ is the mRNA production rate, and $\\gamma_m$ is the\n",
    "mRNA degradation rate.\n",
    "\n",
    "The challenge to evaluate this distribution is that the confluent hypergeometric\n",
    "function $_1F_1$ can be numerically unstable in certain parameter regimes. In a\n",
    "separate notebook, we explored asymptotic approximations to the confluent\n",
    "hypergeometric function, allowing us to tackle this problem. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Here, we will focus\n",
    "on setting up a neural network to perform inference on the parameters of the\n",
    "model.\n",
    "\n",
    "But before getting to that, let's show the steady-state mRNA distribution for\n",
    "a parameter regime to make sure all functions previously-defined are working."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k_on = 4.0\n",
    "k_off = 18.0\n",
    "r_m = 100.0\n",
    "\n",
    "# Define range of mRNA\n",
    "m_range = np.arange(0, 75)\n",
    "\n",
    "# Evaluate the log probability\n",
    "logP = utils.two_state_log_probability(m_range, k_on, k_off, r_m)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Plot the probability\n",
    "ax.step(m_range, np.exp(logP))\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"mRNA copy number\")\n",
    "ax.set_ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks excellent. Let's try again with a more challenging parameter regime."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "k_on = 4.0\n",
    "k_off = 2.0\n",
    "r_m = 8000.0\n",
    "\n",
    "# Define range of mRNA\n",
    "m_range = np.arange(7000, 9000, 25)\n",
    "\n",
    "# Evaluate the log probability\n",
    "logP = utils.two_state_log_probability(m_range, k_on, k_off, r_m)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Plot the probability\n",
    "ax.step(m_range, np.exp(logP) / np.sum(np.exp(logP)))\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"mRNA copy number\")\n",
    "ax.set_ylabel(\"Probability\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, everything looks in order. Let's now move on to setting the `BayesFlow`\n",
    "neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prior distribution function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the function that samples parameters from the prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_fn():\n",
    "    # Prior on k_on rate\n",
    "    k_on = np.abs(rng.normal(1E-4, 10))\n",
    "    # Prior on k_off rate\n",
    "    k_off = np.abs(rng.normal(1E-4, 10))\n",
    "    # Prior on rate of transcription r\n",
    "    r = 1E-3 + np.abs(10 * rng.standard_cauchy())\n",
    "\n",
    "    return np.float32(np.array([k_on, k_off, r]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now define the `BayesFlow` `simulation.Prior` object and sample from the\n",
    "prior distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameter names\n",
    "param_names = ['k_on', 'k_off', 'r']\n",
    "\n",
    "# Define prior simulator\n",
    "prior = bf.simulation.Prior(\n",
    "    prior_fun=prior_fn,\n",
    "    param_names=param_names,\n",
    ")\n",
    "\n",
    "# Draw samples from the prior\n",
    "prior(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we can define the likelihood function. This function takes as input the\n",
    "log parameters and simulates a dataset by sampling from the likelihood function\n",
    "conditioned on the parameters. The likelihood function is given by the steady\n",
    "state mRNA distribution described above in Eq. (1)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the first attempt, we will try to compute the likelihood function by \n",
    "evaluating the log probability of the mRNA copy number given the parameters for\n",
    "a fixed range of mRNA copy numbers. After having these probabilities, we can\n",
    "sample from the distribution to simulate UMI counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_fn(params, n_obs=10_000, m_range=range(0, 2_000)):\n",
    "    # Unpack parameters\n",
    "    k_on, k_off, r = params\n",
    "    # Compute the log probability over m_range\n",
    "    logP = utils.two_state_log_probability(m_range, k_on, k_off, r)\n",
    "    # Convert log probabilities to probabilities\n",
    "    P = np.exp(logP)\n",
    "    # Normalize the probabilities to use as weights. This is necessary because\n",
    "    # of numerical precision issues.\n",
    "    P /= P.sum()\n",
    "    # Generate random samples using these weights\n",
    "    u = np.random.choice(m_range, size=n_obs, p=P)\n",
    "    # Add a 3rd dimension to the array to make output 3D tensor\n",
    "    u = np.expand_dims(u, axis=1)\n",
    "    # Return the samples as float32\n",
    "    return np.float32(u)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's define the `BayesFlow` `simulation.Simulator` object and the generative\n",
    "model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define Likelihood simulator function for BayesFlow\n",
    "simulator = bf.simulation.Simulator(simulator_fun=likelihood_fn)\n",
    "\n",
    "# Build generative model\n",
    "model = bf.simulation.GenerativeModel(prior, simulator)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's time how long it takes to simulate 10 datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%time model_draws = model(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The time varies every time we run this simulation. The precision to compute the\n",
    "confluent hypergeometric function is the main factor that determines the time it\n",
    "takes to simulate the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try simulating a large number of datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of draws\n",
    "n_draws = 100\n",
    "# Draw samples from the generative model\n",
    "%time model_draws = model(n_draws)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's plot the simulated data ECDFs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Loop through the draws and plot the data\n",
    "for m in range(n_draws):\n",
    "    sns.ecdfplot(model_draws[\"sim_data\"][m, :, :].flatten(), ax=ax)\n",
    "\n",
    "# Set x-axis to log scale\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"UMI counts\")\n",
    "ax.set_ylabel(\"ECDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This covers a wide range of mRNA copy numbers. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we define the summary network. For this, we will try the `DeepSet` network\n",
    "architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define summary network as a Deepset\n",
    "summary_net = bf.networks.DeepSet(summary_dim=8)\n",
    "\n",
    "# Simulate a pass through the summary network\n",
    "summary_pass = summary_net(model_draws[\"sim_data\"])\n",
    "\n",
    "summary_pass.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inference network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the inference network, we will use the default `InvertibleNetwork`\n",
    "architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the conditional invertible network with affine coupling layers\n",
    "inference_net = bf.inference_networks.InvertibleNetwork(\n",
    "    num_params=prior(1)[\"prior_draws\"].shape[-1],\n",
    ")\n",
    "\n",
    "inference_net(model_draws['prior_draws'], summary_pass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we are ready to put everything together and train the model.\n",
    "\n",
    "First, we define the `BayesFlow` `amortizer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the amoratizer that combines the summary network and inference\n",
    "# network\n",
    "amortizer = bf.amortizers.AmortizedPosterior(\n",
    "    inference_net, summary_net,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we set the `trainer`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the trainer with the amortizer and generative model\n",
    "trainer = bf.trainers.Trainer(amortizer=amortizer, generative_model=model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of epochs\n",
    "n_epoch = 10\n",
    "# Define the number of iterations per epoch\n",
    "n_iter = 100\n",
    "# Define the batch size\n",
    "batch_size = 128\n",
    "# Define number of validation simulations\n",
    "n_val = 200\n",
    "\n",
    "# Train the model\n",
    "history = trainer.train_online(\n",
    "    epochs=n_epoch,\n",
    "    iterations_per_epoch=n_iter,\n",
    "    batch_size=batch_size,\n",
    "    validation_sims=n_val,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's save the model to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "amortizer.save_weights(\"./two_state_amortizer\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plot_losses(\n",
    "    history[\"train_losses\"], history[\"val_losses\"], moving_average=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sims = trainer.configurator(model(500))\n",
    "z_samples, _ = amortizer(test_sims)\n",
    "f = bf.diagnostics.plot_latent_space_2d(z_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 100 posterior samples for each simulated data set in test_sims\n",
    "posterior_samples = amortizer.sample(test_sims, n_samples=100)\n",
    "f = bf.diagnostics.plot_sbc_histograms(\n",
    "    posterior_samples, test_sims[\"parameters\"], num_bins=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plot_sbc_ecdf(\n",
    "    posterior_samples, test_sims[\"parameters\"], difference=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples = amortizer.sample(test_sims, n_samples=1000)\n",
    "f = bf.diagnostics.plot_recovery(post_samples, test_sims[\"parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plot_z_score_contraction(\n",
    "    post_samples, test_sims[\"parameters\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrappy-jDg5b02t-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
