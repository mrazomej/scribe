{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import utils\n",
    "import io\n",
    "import numpy as np\n",
    "import math\n",
    "import scipy\n",
    "import mpmath\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import bayesflow as bf\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "# Set TensorFlow to use only the CPU\n",
    "tf.config.set_visible_devices([], 'GPU')\n",
    "\n",
    "cor, pal = utils.matplotlib_style()\n",
    "\n",
    "# Set random seed\n",
    "RANDOM_SEED = 42\n",
    "rng = np.random.default_rng(RANDOM_SEED)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-state log "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_fn():\n",
    "    # Prior on k_on rate\n",
    "    k_on = np.abs(rng.normal(1E-4, 10))\n",
    "    # Prior on k_off rate\n",
    "    k_off = np.abs(rng.normal(1E-4, 10))\n",
    "    # Prior on rate of transcription r\n",
    "    r = 1E-3 + np.abs(10 * rng.standard_cauchy())\n",
    "\n",
    "    return np.float32(np.log(np.array([k_on, k_off, r])))\n",
    "\n",
    "\n",
    "# Define parameter names\n",
    "param_names = ['log_k_on', 'log_k_off', 'log_r']\n",
    "\n",
    "# Define prior simulator\n",
    "prior = bf.simulation.Prior(\n",
    "    prior_fun=prior_fn,\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_fn(params, n_obs=10_000, m_range=range(0, 3_000)):\n",
    "    # Unpack parameters\n",
    "    k_on, k_off, r = np.exp(params)\n",
    "    # Compute the log probability over m_range\n",
    "    logP = utils.two_state_log_probability(m_range, k_on, k_off, r)\n",
    "    # Convert log probabilities to probabilities\n",
    "    P = np.exp(logP)\n",
    "    # Normalize the probabilities to use as weights. This is necessary because\n",
    "    # of numerical precision issues.\n",
    "    P /= P.sum()\n",
    "    # Generate random samples using these weights\n",
    "    u = np.random.choice(m_range, size=n_obs, p=P)\n",
    "    # Add a 3rd dimension to the array to make output 3D tensor\n",
    "    u = np.expand_dims(u, axis=1)\n",
    "    # Return the samples as float32\n",
    "    return np.float32(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining the generative model...\")\n",
    "\n",
    "# Define Likelihood simulator function for BayesFlow\n",
    "simulator = bf.simulation.Simulator(simulator_fun=likelihood_fn)\n",
    "\n",
    "# Build generative model\n",
    "model = bf.simulation.GenerativeModel(prior, simulator)\n",
    "\n",
    "# Define summary network as a Deepset\n",
    "summary_net = bf.networks.DeepSet(summary_dim=32)\n",
    "\n",
    "# Define the conditional invertible network with affine coupling layers\n",
    "inference_net = bf.inference_networks.InvertibleNetwork(\n",
    "    num_params=prior(1)[\"prior_draws\"].shape[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of draws\n",
    "n_draws = 50\n",
    "# Draw samples from the generative model\n",
    "model_draws = model(n_draws)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Loop through the draws and plot the data\n",
    "for m in range(n_draws):\n",
    "    sns.ecdfplot(model_draws[\"sim_data\"][m, :, :].flatten(), ax=ax)\n",
    "\n",
    "# Set x-axis to log scale\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"UMI counts\")\n",
    "ax.set_ylabel(\"ECDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the amoratizer that combines the summary network and inference\n",
    "# network\n",
    "amortizer = bf.amortizers.AmortizedPosterior(\n",
    "    inference_net, summary_net,\n",
    ")\n",
    "\n",
    "# Assemble the trainer with the amortizer and generative model\n",
    "trainer = bf.trainers.Trainer(\n",
    "    amortizer=amortizer,\n",
    "    generative_model=model,\n",
    "    checkpoint_path=\"./two_state_log_bayesflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sims = trainer.configurator(model(500))\n",
    "z_samples, _ = amortizer(test_sims)\n",
    "f = bf.diagnostics.plot_latent_space_2d(z_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 100 posterior samples for each simulated data set in test_sims\n",
    "posterior_samples = amortizer.sample(test_sims, n_samples=100)\n",
    "f = bf.diagnostics.plot_sbc_histograms(\n",
    "    posterior_samples,\n",
    "    test_sims[\"parameters\"],\n",
    "    num_bins=10,\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plot_sbc_ecdf(\n",
    "    posterior_samples, \n",
    "    test_sims[\"parameters\"], \n",
    "    difference=True,\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples = amortizer.sample(test_sims, n_samples=1000)\n",
    "f = bf.diagnostics.plot_recovery(\n",
    "    post_samples,\n",
    "    test_sims[\"parameters\"],\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plot_z_score_contraction(\n",
    "    post_samples, test_sims[\"parameters\"], param_names=param_names\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Two-state log constraint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prior_fn():\n",
    "    # Prior on log_k_on rate\n",
    "    log_k_on = np.log(np.abs(rng.normal(1E-4, 10)))\n",
    "    # Prior on log_k_off\n",
    "    log_k_off = np.log(np.abs(rng.normal(1E-4, 10)))\n",
    "    # Prior on log(r / k_off)\n",
    "    log_r_k_off = np.log(1E-3 + np.abs(rng.standard_cauchy()))\n",
    "    # Prior on rate of transcription r\n",
    "    log_r = log_r_k_off + log_k_off\n",
    "\n",
    "    return np.float32(np.array([log_k_on, log_k_off, log_r]))\n",
    "\n",
    "\n",
    "# Define parameter names\n",
    "param_names = ['log_k_on', 'log_k_off', 'log_r']\n",
    "\n",
    "# Define prior simulator\n",
    "prior = bf.simulation.Prior(\n",
    "    prior_fun=prior_fn,\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def likelihood_fn(params, n_obs=10_000, m_range=range(0, 3_000)):\n",
    "    # Unpack parameters\n",
    "    k_on, k_off, r = np.exp(params)\n",
    "    # Compute the log probability over m_range\n",
    "    logP = utils.two_state_log_probability(m_range, k_on, k_off, r)\n",
    "    # Convert log probabilities to probabilities\n",
    "    P = np.exp(logP)\n",
    "    # Normalize the probabilities to use as weights. This is necessary because\n",
    "    # of numerical precision issues.\n",
    "    P /= P.sum()\n",
    "    # Generate random samples using these weights\n",
    "    u = np.random.choice(m_range, size=n_obs, p=P)\n",
    "    # Add a 3rd dimension to the array to make output 3D tensor\n",
    "    u = np.expand_dims(u, axis=1)\n",
    "    # Return the samples as float32\n",
    "    return np.float32(u)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Defining the generative model...\")\n",
    "\n",
    "# Define Likelihood simulator function for BayesFlow\n",
    "simulator = bf.simulation.Simulator(simulator_fun=likelihood_fn)\n",
    "\n",
    "# Build generative model\n",
    "model = bf.simulation.GenerativeModel(prior, simulator)\n",
    "\n",
    "# Define summary network as a Deepset\n",
    "summary_net = bf.networks.DeepSet(summary_dim=32)\n",
    "\n",
    "# Define the conditional invertible network with affine coupling layers\n",
    "inference_net = bf.inference_networks.InvertibleNetwork(\n",
    "    num_params=prior(1)[\"prior_draws\"].shape[-1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the number of draws\n",
    "n_draws = 50\n",
    "# Draw samples from the generative model\n",
    "model_draws = model(n_draws)\n",
    "\n",
    "# Initialize figure\n",
    "fig, ax = plt.subplots(1, 1, figsize=(1.75, 1.5))\n",
    "\n",
    "# Loop through the draws and plot the data\n",
    "for m in range(n_draws):\n",
    "    sns.ecdfplot(model_draws[\"sim_data\"][m, :, :].flatten(), ax=ax)\n",
    "\n",
    "# Set x-axis to log scale\n",
    "ax.set_xscale(\"log\")\n",
    "\n",
    "# Label axis\n",
    "ax.set_xlabel(\"UMI counts\")\n",
    "ax.set_ylabel(\"ECDF\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assemble the amoratizer that combines the summary network and inference\n",
    "# network\n",
    "amortizer = bf.amortizers.AmortizedPosterior(\n",
    "    inference_net, summary_net,\n",
    ")\n",
    "\n",
    "# Assemble the trainer with the amortizer and generative model\n",
    "trainer = bf.trainers.Trainer(\n",
    "    amortizer=amortizer,\n",
    "    generative_model=model,\n",
    "    checkpoint_path=\"./two_state_log_constraint_bayesflow\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_sims = trainer.configurator(model(500))\n",
    "z_samples, _ = amortizer(test_sims)\n",
    "f = bf.diagnostics.plot_latent_space_2d(z_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Obtain 100 posterior samples for each simulated data set in test_sims\n",
    "posterior_samples = amortizer.sample(test_sims, n_samples=100)\n",
    "f = bf.diagnostics.plot_sbc_histograms(\n",
    "    posterior_samples,\n",
    "    test_sims[\"parameters\"],\n",
    "    num_bins=10,\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = bf.diagnostics.plot_sbc_ecdf(\n",
    "    posterior_samples,\n",
    "    test_sims[\"parameters\"],\n",
    "    difference=True,\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "post_samples = amortizer.sample(test_sims, n_samples=1000)\n",
    "f = bf.diagnostics.plot_recovery(\n",
    "    post_samples,\n",
    "    test_sims[\"parameters\"],\n",
    "    param_names=param_names,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrappy-jDg5b02t-py3.11",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
