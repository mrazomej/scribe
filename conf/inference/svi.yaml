# @package inference
# Stochastic Variational Inference configuration
#
# SVI optimizes the ELBO (Evidence Lower Bound) using gradient-based methods.
# This configuration supports early stopping to automatically detect convergence.

method: svi

# Maximum number of optimization steps
# Training may stop earlier if early stopping is enabled and convergence is detected
n_steps: 50_000

# Mini-batch size for stochastic optimization
# null = use full dataset (batch gradient descent)
# Set to a value (e.g., 256, 512) for large datasets
batch_size: null

# Use numerically stable parameter updates
# When true, handles NaN/Inf gracefully
stable_update: true

# Early stopping configuration
# Monitors loss and stops when improvement plateaus
# Supports Orbax checkpointing for resumable training
early_stopping:
  # Enable/disable early stopping
  # Set to false to always run for n_steps
  enabled: true
  
  # Number of steps without improvement before stopping
  # Higher values = more tolerance for temporary loss increases
  patience: 500
  
  # Minimum relative improvement (as percentage) to qualify as progress
  # Computed as: 100 * (best_loss - smoothed_loss) / best_loss
  # Default 0.01 means 0.01% improvement required
  # This scales automatically with loss magnitude (dataset size)
  min_delta_pct: 0.01
  
  # How often to check for convergence (in steps)
  # Lower = more responsive but slightly more overhead
  check_every: 100
  
  # Window size for computing smoothed (moving average) loss
  # Larger windows reduce noise but respond slower
  smoothing_window: 100
  
  # Restore parameters from best checkpoint when stopping
  # If false, uses parameters from the final step
  restore_best: true
  
  # Resume from checkpoint if one exists
  # Checkpoints are automatically saved to the Hydra output directory
  # Set to false to start fresh training
  resume: true
