# @package inference
# Stochastic Variational Inference configuration
#
# SVI optimizes the ELBO (Evidence Lower Bound) using gradient-based methods.
# This configuration supports early stopping to automatically detect convergence.

method: svi

# Run in float64 (double) precision
# null = use method default (false for SVI). Set to true for edge cases
# requiring higher precision (extreme probabilities, model comparison).
enable_x64: null

# Maximum number of optimization steps
# Training may stop earlier if early stopping is enabled and convergence is detected
n_steps: 50_000

# Mini-batch size for stochastic optimization
# null = use full dataset (batch gradient descent)
# Set to a value (e.g., 256, 512) for large datasets
batch_size: null

# Use numerically stable parameter updates
# When true, handles NaN/Inf gracefully
stable_update: true

# Emit plain-text progress lines suitable for non-interactive logs (e.g., SLURM)
# When true, prints an update every max(1, n_steps // 20) steps
log_progress_lines: false

# Empirical mixing weights for mixture models
# SVI-learned Dirichlet mixing weights are practically non-identifiable in
# high-dimensional mixture models.  When true (default), the pipeline replaces
# them with data-driven weights from the conditional posterior Dir(alpha_0 +
# N_soft) after inference completes.  Set to false to keep the raw SVI weights.
# Only has effect for mixture models (n_components >= 2).
empirical_mixing: true

# Early stopping configuration
# Monitors loss and stops when improvement plateaus
# Supports Orbax checkpointing for resumable training
early_stopping:
  # Enable/disable early stopping
  # Set to false to always run for n_steps
  enabled: false
  
  # Number of steps without improvement before stopping
  # Higher values = more tolerance for temporary loss increases
  patience: 500
  
  # Minimum absolute improvement in loss to qualify as progress
  # Used when min_delta_pct is not specified
  min_delta: 1.0
  
  # Minimum relative improvement (as percentage) to qualify as progress
  # If specified, takes precedence over min_delta
  # Computed as: 100 * (best_loss - smoothed_loss) / best_loss
  # E.g., 0.01 means 0.01% improvement required
  # Set to null to use min_delta instead
  min_delta_pct: null
  
  # How often to check for early stopping (in steps)
  # Loss is stored every step; this controls how often we evaluate convergence
  check_every: 100
  
  # Warmup steps before early stopping is activated
  # During warmup, loss is tracked but stopping criteria are not evaluated
  # Set to 0 to disable warmup
  warmup: 5000
  
  # Window size for computing smoothed (moving average) loss
  # Number of steps to average for smoothed loss calculation
  smoothing_window: 100
  
  # How often to save checkpoints (in steps)
  # Checkpoints are only saved when improvement is detected AND
  # at least this many steps have passed since the last checkpoint
  checkpoint_every: 2500
  
  # Restore parameters from best checkpoint when stopping
  # If false, uses parameters from the final step
  restore_best: true
  
  # Resume from checkpoint if one exists
  # Checkpoints are automatically saved to the Hydra output directory
  # Set to false to start fresh training
  resume: true
