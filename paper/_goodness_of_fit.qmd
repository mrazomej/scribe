---
editor:
    render-on-save: true
# bibliography: references.bib
csl: ieee.csl
---

## Goodness-of-Fit via Randomized Quantile Residuals {#sec-gof}

The differential expression framework developed in previous sections and the
model comparison tools of @sec-model-comparison share a common requirement: the
fitted model must adequately describe the data for each gene before downstream
inferences can be trusted. When comparing two conditions, genes whose
observations are poorly explained by the model may produce unreliable DE
statistics---regardless of whether those statistics are computed via the
parametric Gaussian path or the empirical Monte Carlo path.

In this section, we develop a principled, per-gene diagnostic for absolute model
fit based on **randomized quantile residuals** (RQR) @dunn1996. Unlike the
model *comparison* criteria of @sec-model-comparison (WAIC, PSIS-LOO), which
rank two or more competing models relative to each other, RQR assesses how well
a *single* fitted model describes each gene's count distribution. The key
property that makes RQR attractive is **expression-scale invariance**: the
diagnostic maps every gene---regardless of its expression level---onto a common
standard-normal reference distribution, eliminating the confound between
expression magnitude and fit quality that plagues raw log-likelihood-based
metrics.

### Motivation: why raw log-likelihood is not sufficient {#sec-gof-motivation}

A natural candidate for a per-gene fit metric is the gene-level expected log
predictive density (elpd) defined in @sec-mc-gene-level:

$$
\text{elpd}_g = \log\!\left(\frac{1}{S}\sum_{s=1}^S
\exp\!\left(\sum_{c=1}^C \log p(u_{gc} \mid \theta^{(s)})\right)\right)
- p_{\text{waic},g},
$${#eq-gof-gene-elpd}

where $p_{\text{waic},g} = \mathrm{Var}_s\!\left(\sum_c \log p(u_{gc} \mid
\theta^{(s)})\right)$ is the per-gene WAIC penalty. While $\text{elpd}_g$ is a
proper scoring rule, its magnitude depends on two distinct factors: (1) how well
the model fits gene $g$, and (2) the expression level and distributional
complexity of gene $g$. Highly expressed genes naturally contribute larger
absolute log-likelihoods per cell simply because the negative binomial mass
function evaluated at large counts produces different baseline values than at
small counts. As a result, the raw elpd confounds "bad fit" with "different
expression level," making it impossible to set a single threshold across genes
without an ad-hoc regression correction whose functional form would itself need
justification.

What we need is a diagnostic that is **self-normalizing**: a metric whose null
distribution under a well-specified model is the same for every gene, regardless
of expression level, dispersion, or number of cells. Randomized quantile
residuals provide exactly this property.

### The probability integral transform {#sec-gof-pit}

We begin with the classical **probability integral transform** (PIT), which
provides the theoretical foundation for RQR. The result is elementary but
central.

#### Continuous case

**Theorem (PIT).** Let $Y$ be a continuous random variable with cumulative
distribution function $F$. Then

$$
V = F(Y) \sim \text{Uniform}(0, 1).
$${#eq-gof-pit-continuous}

*Proof.* Since $F$ is continuous and strictly increasing on the support of $Y$,
it has an inverse $F^{-1}$. For any $t \in [0, 1]$,

$$
\Pr(V \leq t) = \Pr(F(Y) \leq t) = \Pr\bigl(Y \leq F^{-1}(t)\bigr) =
F\!\left(F^{-1}(t)\right) = t,
$${#eq-gof-pit-proof}

which is the CDF of the Uniform$(0,1)$ distribution. $\square$

The PIT provides a universal calibration check: if a model $F$ is correct for
the data, then the transformed values $F(y_1), F(y_2), \ldots$ should be i.i.d.
Uniform$(0,1)$.

#### The discrete complication

When $Y$ takes values in a discrete set (as with UMI counts), the CDF $F$ is a
step function that jumps at each support point. In this case, $F(Y)$ takes only
finitely many values---one at each jump---and **is not uniformly distributed**.
Concretely, if $Y \sim F$ is a discrete random variable with probability mass
function $f$, then

$$
\Pr\bigl(F(Y) = F(k)\bigr) = f(k) \quad
\text{for each } k \text{ in the support.}
$${#eq-gof-pit-discrete-problem}

The image of $F$ applied to the support is a discrete set $\{F(k) : k \in
\mathcal{S}\}$, which cannot be uniformly distributed on $[0, 1]$.

#### The Dunn--Smyth randomization

Dunn and Smyth @dunn1996 resolved this discrete-PIT failure with a simple
randomization. Define the **lower** and **upper** cumulative probabilities at
the observed count $y$:

$$
a(y) = F(y - 1) = \Pr(Y < y), \qquad
b(y) = F(y) = \Pr(Y \leq y).
$${#eq-gof-dunn-smyth-bounds}

Note that $a(y) \leq b(y)$, with equality only when $\Pr(Y = y) = 0$ (which
does not occur for points in the support). For the special case $y = 0$, we
adopt the convention $F(-1) = 0$, so $a(0) = 0$.

The **randomized probability integral transform** is defined as

$$
V = a(Y) + W \cdot \bigl(b(Y) - a(Y)\bigr),
\qquad W \sim \text{Uniform}(0, 1),
$${#eq-gof-randomized-pit}

where $W$ is an independent Uniform$(0,1)$ draw. Equivalently, $V \sim
\text{Uniform}\bigl(a(Y),\, b(Y)\bigr)$ conditional on $Y$.

**Claim.** Under the true model, $V \sim \text{Uniform}(0, 1)$.

*Proof.* We verify $\Pr(V \leq t) = t$ for all $t \in [0, 1]$. By the law of
total probability, conditioning on $Y$:

$$
\Pr(V \leq t) = \sum_{k \in \mathcal{S}} \Pr(V \leq t \mid Y = k) \cdot
\Pr(Y = k).
$${#eq-gof-rpit-total-prob}

When $Y = k$, we have $V \mid Y{=}k \sim \text{Uniform}(a_k, b_k)$ where
$a_k = F(k-1)$ and $b_k = F(k)$. Therefore

$$
\Pr(V \leq t \mid Y = k) =
\begin{cases}
0 & \text{if } t < a_k, \\[4pt]
\displaystyle\frac{t - a_k}{b_k - a_k} & \text{if } a_k \leq t < b_k, \\[6pt]
1 & \text{if } t \geq b_k.
\end{cases}
$${#eq-gof-rpit-conditional}

Now partition the support $\mathcal{S} = \{k_0, k_1, k_2, \ldots\}$ (with
$k_0 = 0$ for count data) and note that $b_{k_j} = a_{k_{j+1}}$ (the CDF is
right-continuous). For a given $t$, there is a unique index $j^*$ such that
$a_{k_{j^*}} \leq t < b_{k_{j^*}}$ (unless $t$ lands exactly on a boundary,
which occurs with probability zero under the continuous $W$). Substituting into
@eq-gof-rpit-total-prob:

$$
\Pr(V \leq t) = \sum_{j < j^*} f(k_j) \cdot 1
+ f(k_{j^*}) \cdot \frac{t - a_{k_{j^*}}}{b_{k_{j^*}} - a_{k_{j^*}}}
+ \sum_{j > j^*} f(k_j) \cdot 0.
$${#eq-gof-rpit-expansion}

The first sum equals $F(k_{j^*} - 1) = a_{k_{j^*}}$. Since
$b_{k_{j^*}} - a_{k_{j^*}} = f(k_{j^*})$, the middle term simplifies to
$t - a_{k_{j^*}}$. Therefore

$$
\Pr(V \leq t) = a_{k_{j^*}} + (t - a_{k_{j^*}}) = t.
\quad \square
$${#eq-gof-rpit-qed}

#### The normal-scale transform

Having established that $V \sim \text{Uniform}(0, 1)$ under the true model, we
apply the **standard normal quantile function** $\Phi^{-1}$ to obtain
**randomized quantile residuals**:

$$
Q = \Phi^{-1}(V).
$${#eq-gof-rqr-definition}

Since $\Phi^{-1}$ maps Uniform$(0,1)$ to $\mathcal{N}(0,1)$, we have

$$
Q \sim \mathcal{N}(0, 1) \quad \text{under the true model.}
$${#eq-gof-rqr-null}

This is the key result: under the correctly specified model, the residuals
$Q$ are standard normal, regardless of the original distribution family,
its parameters, or the expression level of the gene.

### Application to the negative binomial model {#sec-gof-nb}

We now specialize the general RQR framework to the negative binomial (NB)
likelihood used throughout this work.

#### Single-component model

Consider cell $c$ and gene $g$ with observed UMI count $u_{gc}$ and NB
parameters $(r_g, \hat{p})$, where $r_g > 0$ is the dispersion and
$\hat{p} \in (0, 1)$ is the probability parameter defined in
@sec-dirichlet-multinomial. The NB probability mass function is

$$
\Pr(U = u \mid r_g, \hat{p}) = \binom{u + r_g - 1}{u}\,
\hat{p}^{r_g}\, (1 - \hat{p})^u,
\qquad u = 0, 1, 2, \ldots
$${#eq-gof-nb-pmf}

The cumulative distribution function is

$$
F_{\text{NB}}(u \mid r_g, \hat{p}) = \Pr(U \leq u \mid r_g, \hat{p}) =
I_{\hat{p}}(r_g,\, u + 1),
$${#eq-gof-nb-cdf}

where $I_x(a, b)$ denotes the **regularized incomplete beta function**,

$$
I_x(a, b) = \frac{B(x;\, a, b)}{B(a, b)} =
\frac{1}{B(a, b)}\int_0^x t^{a-1}(1-t)^{b-1}\,dt.
$${#eq-gof-regularized-beta}

This representation follows from the well-known identity connecting the NB CDF
to the beta distribution CDF; it is the form implemented in standard numerical
libraries including NumPyro.

The RQR for cell $c$ and gene $g$ proceeds as follows:

1. **Lower bound.** Compute $a_{gc} = F_{\text{NB}}(u_{gc} - 1 \mid r_g, \hat{p})$,
   with the convention that $F_{\text{NB}}(-1 \mid \cdot) = 0$ (i.e., when
   $u_{gc} = 0$, set $a_{gc} = 0$).

2. **Upper bound.** Compute $b_{gc} = F_{\text{NB}}(u_{gc} \mid r_g, \hat{p})$.

3. **Randomize.** Draw $w_{gc} \sim \text{Uniform}(0, 1)$ independently and set

$$
v_{gc} = a_{gc} + w_{gc} \cdot (b_{gc} - a_{gc}).
$${#eq-gof-nb-randomize}

4. **Transform.** Compute the quantile residual

$$
q_{gc} = \Phi^{-1}(v_{gc}).
$${#eq-gof-nb-transform}

Under the correctly specified model, $q_{gc} \sim \mathcal{N}(0, 1)$ for all
$c = 1, \ldots, C$ and $g = 1, \ldots, G$, regardless of the values of $r_g$,
$\hat{p}$, or the expression level $\mu_g = r_g (1 - \hat{p}) / \hat{p}$.

**Numerical safeguard.** To prevent $\Phi^{-1}(v)$ from returning $\pm\infty$
when $v$ is exactly 0 or 1 (which can occur in finite-precision arithmetic),
we clip $v_{gc}$ to the interval $(\epsilon, 1 - \epsilon)$ for a small
$\epsilon > 0$ (e.g., $\epsilon = 10^{-6}$) before applying the inverse normal
CDF.

### Extension to mixture models {#sec-gof-mixture}

In the mixture models developed in this work, each cell $c$ is modeled as a
draw from one of $K$ components, each with its own gene-specific parameters.
The marginal distribution of the UMI count $u_{gc}$ for cell $c$ and gene $g$
is

$$
\Pr(U_{gc} = u) = \sum_{k=1}^K \pi_k \,
\Pr(U = u \mid r_{gk}, \hat{p}_k),
$${#eq-gof-mixture-pmf}

where $\pi_k$ is the mixing weight for component $k$ (with $\sum_k \pi_k = 1$),
and $(r_{gk}, \hat{p}_k)$ are the NB parameters for gene $g$ under component $k$.
In models with cell-specific assignment probabilities, $\pi_k$ may be replaced
by $\pi_{ck}$; the development below applies identically.

The **marginal CDF** of the mixture is the correspondingly weighted sum of
component CDFs:

$$
F_{gc}(u) = \sum_{k=1}^K \pi_k \, F_{\text{NB}}(u \mid r_{gk}, \hat{p}_k).
$${#eq-gof-mixture-cdf}

This follows from the linearity of the CDF operator applied to @eq-gof-mixture-pmf.

**Critical point.** When computing RQR for mixture models, one must use the
**marginal** CDF in @eq-gof-mixture-cdf, not the component-conditional CDF
$F_{\text{NB}}(u \mid r_{gk}, \hat{p}_k)$ for any single component $k$. Using a
component-conditional CDF would condition on the latent assignment $z_c = k$,
which is not observed. The marginal CDF integrates out the latent variable
and correctly represents the model's predictive distribution for each cell.

Given the marginal CDF, the RQR procedure is identical to the single-component
case:

1. $a_{gc} = F_{gc}(u_{gc} - 1)$, with $F_{gc}(-1) = 0$.

2. $b_{gc} = F_{gc}(u_{gc})$.

3. $v_{gc} = a_{gc} + w_{gc} \cdot (b_{gc} - a_{gc})$, \quad $w_{gc} \sim
   \text{Uniform}(0, 1)$.

4. $q_{gc} = \Phi^{-1}(v_{gc})$.

Under the correctly specified mixture, $q_{gc} \sim \mathcal{N}(0, 1)$.

### Integrating over posterior uncertainty {#sec-gof-posterior}

In a Bayesian workflow, the model parameters $\theta = \{r_g, \hat{p}\}$ (or
$\theta = \{r_{gk}, \hat{p}_k, \pi_k\}$ for mixtures) are not known exactly; we
have a posterior distribution $p(\theta \mid y)$ represented by $S$ posterior
samples $\{\theta^{(s)}\}_{s=1}^S$.

#### MAP-based residuals (practical default)

The simplest approach is to evaluate the RQR at the **maximum a posteriori**
(MAP) estimates $\hat{\theta}$, producing a single residual matrix
$\mathbf{Q} = [q_{gc}]$ of shape $(C, G)$. This is computationally cheap
(one CDF evaluation per cell-gene pair) and sufficient for gene filtering in
most practical settings.

The MAP-based approach can be viewed as a plug-in approximation that replaces
the posterior $p(\theta \mid y)$ with a point mass at $\hat{\theta}$. Since
$\hat{\theta}$ is typically a good summary of the posterior (especially after
SVI with a well-fitted guide), the resulting residuals are informative about
model fit.

#### Posterior-averaged residuals (more principled)

For a fuller integration of posterior uncertainty, one can compute residuals
for each posterior sample $s$ and then aggregate. Specifically:

1. For each sample $s = 1, \ldots, S$ and each cell-gene pair $(c, g)$, compute

$$
q_{gc}^{(s)} = \Phi^{-1}\!\left(v_{gc}^{(s)}\right),
$${#eq-gof-posterior-residual}

   where $v_{gc}^{(s)}$ is the randomized PIT under parameters $\theta^{(s)}$.
   Use the **same** uniform draw $w_{gc}$ across all posterior samples to reduce
   Monte Carlo noise (the randomization addresses discreteness, not posterior
   uncertainty).

2. Compute per-gene summary statistics (see @sec-gof-scores) for each sample
   $s$, obtaining a distribution of summary statistics across the posterior.

3. Report the **posterior median** (or mean) of each summary statistic as the
   final per-gene diagnostic.

The posterior-averaged approach correctly accounts for parameter uncertainty:
a gene whose fit quality varies substantially across posterior samples is
flagged as uncertain regardless of the MAP fit.

**Practical recommendation.** For gene filtering in the DE pipeline, MAP-based
residuals provide a computationally efficient default. Reserve posterior-averaged
residuals for detailed model-checking analyses where thorough calibration
assessment is needed.

### Per-gene summary statistics {#sec-gof-scores}

Given the residual matrix $\mathbf{Q} = [q_{gc}]$ of shape $(C, G)$, we
summarize the goodness of fit for each gene $g$ via statistics that measure
departures from the standard-normal reference. Under a correctly specified
model, $\{q_{1g}, q_{2g}, \ldots, q_{Cg}\}$ should be approximately
i.i.d. $\mathcal{N}(0, 1)$.

#### Location miscalibration

The sample mean of the residuals for gene $g$,

$$
\bar{q}_g = \frac{1}{C} \sum_{c=1}^C q_{gc},
$${#eq-gof-score-mean}

measures whether the model systematically over- or under-predicts gene $g$
across cells. Under the null, $\bar{q}_g$ has expectation 0 and standard
deviation $1/\sqrt{C}$, so values $|\bar{q}_g| \gg 1/\sqrt{C}$ indicate
location miscalibration. For typical single-cell datasets with $C$ in the
hundreds or thousands, even modest systematic biases produce detectable
$\bar{q}_g$ values.

#### Scale miscalibration (primary diagnostic)

The sample variance of the residuals for gene $g$,

$$
s_g^2 = \frac{1}{C - 1} \sum_{c=1}^C (q_{gc} - \bar{q}_g)^2,
$${#eq-gof-score-variance}

is the most informative single diagnostic. Under the null, $s_g^2$ has
expectation 1 and approximate distribution $(C-1)s_g^2 \sim \chi^2(C-1)$,
so the standard deviation of $s_g^2$ around 1 is $\sqrt{2/(C-1)}$, which is
small for large $C$.

**Interpretation of departures:**

- $s_g^2 \gg 1$: the model **underestimates the variability** of gene $g$
  across cells. The NB dispersion $r_g$ is too large (too little overdispersion)
  or the gene exhibits extra variation (e.g., zero-inflation, bimodality) not
  captured by the model.
- $s_g^2 \ll 1$: the model **overestimates the variability**, assigning too much
  uncertainty to gene $g$. This can occur when the prior on $r_g$ is too
  diffuse or when the gene is well-described by a simpler model (e.g., Poisson).

We recommend $s_g^2$ as the primary scalar summary for gene filtering because
(a) it captures the most common failure mode (underdispersion / overdispersion
mismatch) and (b) its null distribution is sharply concentrated around 1 for
large $C$.

#### Tail excess

The fraction of residuals exceeding 2 in absolute value,

$$
\tau_g = \frac{1}{C} \sum_{c=1}^C \mathbf{1}\bigl[|q_{gc}| > 2\bigr]
- 0.0455,
$${#eq-gof-score-tail}

where $0.0455 = 2\bigl(1 - \Phi(2)\bigr)$ is the expected tail fraction
under $\mathcal{N}(0, 1)$. The subtraction centers $\tau_g$ at zero under the
null. Large positive $\tau_g$ indicates an excess of outlier cells for gene $g$;
large negative $\tau_g$ (rare in practice) would indicate lighter tails than
expected.

#### Kolmogorov--Smirnov distance

The KS statistic provides an omnibus measure of departure from standard
normality:

$$
D_g = \sup_{t \in \mathbb{R}} \bigl|\hat{F}_g(t) - \Phi(t)\bigr|,
$${#eq-gof-score-ks}

where $\hat{F}_g$ is the empirical CDF of $\{q_{1g}, \ldots, q_{Cg}\}$ and
$\Phi$ is the standard normal CDF. The KS distance is sensitive to any
departure from the reference distribution. Under the null, the expected KS
distance scales as $O(1/\sqrt{C})$; its distribution is tabulated (Kolmogorov
distribution) and does not depend on the parameters of the model.

**Remark.** In the large-$C$ regime typical of single-cell data, the KS test
becomes extremely powerful and will flag even minor, practically irrelevant
departures from normality. We therefore recommend using $D_g$ as a ranking
criterion (genes with larger $D_g$ are more suspect) rather than applying a
formal significance threshold. The variance-based summary $s_g^2$ is better
suited for threshold-based filtering.

### Gene filtering for differential expression {#sec-gof-filtering}

The per-gene summary statistics in @sec-gof-scores provide the building blocks
for a goodness-of-fit gene mask that can be used directly in the DE pipeline.
Recall from the empirical DE framework that the `gene_mask` parameter controls
which genes are analyzed individually versus pooled into an "other" category.

#### Constructing the mask

The simplest and most interpretable filter is based on the residual variance
$s_g^2$. Since both over- and underdispersion relative to the model indicate
misfit (see @sec-gof-scores), the mask should be **two-sided**:

$$
m_g = \mathbf{1}\!\left[\tau_{\text{lo}} < s_g^2 < \tau_{\text{hi}}\right],
$${#eq-gof-mask-variance}

where $\tau_{\text{lo}} < 1$ and $\tau_{\text{hi}} > 1$ are user-specified
bounds. Genes with $m_g = 1$ (residual variance within tolerance) are retained
for individual DE analysis; genes with $m_g = 0$ are pooled into the aggregate
"other" category.

The upper bound $\tau_{\text{hi}}$ catches genes whose variability the model
**underestimates** (e.g., missing zero-inflation or bimodality). The lower
bound $\tau_{\text{lo}}$ catches genes where the model **overestimates**
variability (e.g., the prior on dispersion is too diffuse, or the model is
substantially more complex than the data warrants).

More conservative filtering can incorporate additional criteria:

$$
m_g = \mathbf{1}\!\left[\tau_{\text{lo}} < s_g^2 < \tau_{\text{hi}}\right]
\cdot \mathbf{1}\!\left[D_g < \tau_{\text{KS}}\right],
$${#eq-gof-mask-combined}

requiring both the variance and the KS distance to be within their respective
bounds.

#### Threshold selection

As with the Gaussianity diagnostics in @sec-diffexp-nonparametric, we advocate
a descriptive-threshold approach rather than formal hypothesis testing.
Reasonable defaults are:

- $\tau_{\text{lo}} = 0.5$: retains genes whose residual variance is at most
  50% below the null expectation, flagging severe overestimation of
  variability.
- $\tau_{\text{hi}} = 1.5$: retains genes whose residual variance is at most
  50% above the null expectation.  Both thresholds are deliberately generous;
  they flag only genes with substantial fit problems.
- $\tau_{\text{KS}}$: if used, a quantile-based cutoff (e.g., the 95th
  percentile of $D_g$ across genes) ensures a consistent fraction of genes pass
  regardless of dataset size.

These thresholds can be calibrated on held-out data or tuned via posterior
predictive checks on a subset of genes.

#### Relationship to expression-based filtering

The goodness-of-fit mask @eq-gof-mask-variance complements rather than replaces
the expression-based mask from `compute_expression_mask`. In practice, genes
with very low expression will often have high residual variance (because the NB
CDF for sparse data is coarse-grained), so the two filters overlap
substantially. However, the goodness-of-fit mask also catches highly expressed
genes that are poorly fit due to zero-inflation, bimodality, or other
departures from the NB family---a class of genes that the expression-based
filter would never flag.

### Computational considerations {#sec-gof-computation}

#### Cost analysis

The dominant cost is evaluating the NB CDF $F_{\text{NB}}(u \mid r, \hat{p})$ for
each cell-gene pair. Each CDF evaluation requires one regularized incomplete
beta function call. Let $C$ denote the number of cells, $G$ the number of
genes, $K$ the number of mixture components, and $S$ the number of posterior
samples.

**MAP-based residuals** (single evaluation):

| **Operation** | **Complexity** | **Memory** |
| --- | --- | --- |
| NB CDF (upper and lower) | $O(CGK)$ | $O(CG)$ |
| Randomization and $\Phi^{-1}$ | $O(CG)$ | $O(CG)$ |
| Per-gene summaries | $O(CG)$ | $O(G)$ |
| **Total** | $O(CGK)$ | $O(CG)$ |

For single-component models ($K = 1$), the cost is $O(CG)$. For a typical
dataset with $C = 5{,}000$ cells and $G = 2{,}000$ genes, this amounts to
$10^7$ CDF evaluations, which completes in seconds on GPU using JAX-accelerated
regularized beta functions.

**Posterior-averaged residuals** ($S$ evaluations):

The cost scales linearly with the number of posterior samples: $O(CGKS)$. For
$S = 500$, this represents a 500$\times$ increase. To manage memory, the
computation can be chunked across posterior samples, evaluating $S_{\text{chunk}}$
samples at a time and accumulating the per-gene summary statistics
incrementally.

#### Implementation via NumPyro

The NB CDF in @eq-gof-nb-cdf is computed via JAX's `jax.lax.betainc`, which
evaluates the regularized incomplete beta function $I_{\hat{p}}(r_g, u+1)$
directly. Note that NumPyro's `NegativeBinomialProbs(total_count, probs)`
parameterizes `probs` as the complement of our $\hat{p}$ (i.e.,
`probs` $= 1 - \hat{p}$), so care must be taken when passing parameters.
The inverse normal CDF $\Phi^{-1}$ is available as
`jax.scipy.stats.norm.ppf`. Both operations are JIT-compilable and support
automatic vectorization via `jax.vmap`, enabling efficient batch computation
over the full $(C, G)$ count matrix.
